{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has 25 pages. Starting OCR extraction...\n",
      "Processing page 1...\n",
      "Processing page 2...\n",
      "Processing page 3...\n",
      "Processing page 4...\n",
      "Processing page 5...\n",
      "Processing page 6...✅ Completed page 5\n",
      "\n",
      "Processing page 7...\n",
      "✅ Completed page 1\n",
      "Processing page 8...✅ Completed page 3\n",
      "\n",
      "Processing page 9...✅ Completed page 2\n",
      "\n",
      "Processing page 10...✅ Completed page 4\n",
      "\n",
      "Processing page 11...\n",
      "✅ Completed page 8\n",
      "Processing page 12...✅ Completed page 10\n",
      "\n",
      "Processing page 13...\n",
      "✅ Completed page 7\n",
      "Processing page 14...\n",
      "✅ Completed page 11\n",
      "Processing page 15...✅ Completed page 12\n",
      "\n",
      "Processing page 16...✅ Completed page 6\n",
      "\n",
      "Processing page 17...\n",
      "✅ Completed page 14\n",
      "Processing page 18...✅ Completed page 15\n",
      "\n",
      "Processing page 19...✅ Completed page 16\n",
      "\n",
      "Processing page 20...\n",
      "✅ Completed page 17\n",
      "Processing page 21...\n",
      "✅ Completed page 9\n",
      "Processing page 22...✅ Completed page 13\n",
      "\n",
      "Processing page 23...✅ Completed page 20\n",
      "\n",
      "Processing page 24...✅ Completed page 18\n",
      "\n",
      "Processing page 25...✅ Completed page 19\n",
      "\n",
      "✅ Completed page 22\n",
      "✅ Completed page 21\n",
      "✅ Completed page 23\n",
      "✅ Completed page 25\n",
      "✅ Completed page 24\n",
      "\n",
      "OCR extraction complete!\n",
      "- Individual page files saved in: EBR-06-922947-PUR-HID2007_922945_235_A Prep and  Sartobind Q_ocr_output\\pages\n",
      "- Combined OCR text saved to: EBR-06-922947-PUR-HID2007_922945_235_A Prep and  Sartobind Q_ocr_output\\combined_ocr.txt\n",
      "\n",
      "Processing completed in 151.19 seconds\n",
      "Combined OCR text saved to: EBR-06-922947-PUR-HID2007_922945_235_A Prep and  Sartobind Q_ocr_output\\combined_ocr.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "import openai\n",
    " \n",
    "# Configuration\n",
    "OPENAI_API_KEY = \"YOUR API KEY\"  # Replace with your OpenAI API key\n",
    "MODEL = \"gpt-4.1\"  # or another vision model\n",
    " \n",
    "def setup_output_directory(pdf_path):\n",
    "    \"\"\"Create output directories for OCR results\"\"\"\n",
    "    base_name = os.path.basename(pdf_path).rsplit('.', 1)[0]\n",
    "    output_dir = f\"{base_name}_ocr_output\"\n",
    "    pages_dir = os.path.join(output_dir, \"pages\")\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(pages_dir, exist_ok=True)\n",
    "    return output_dir, pages_dir\n",
    " \n",
    "def extract_page_as_image(pdf_path, page_num, dpi=300):\n",
    "    \"\"\"Extract a single page from PDF as an image\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(page_num)\n",
    "    # Higher resolution for better OCR quality\n",
    "    pix = page.get_pixmap(matrix=fitz.Matrix(dpi/72, dpi/72))\n",
    "    img_data = pix.pil_tobytes(format=\"JPEG\")\n",
    "    image = Image.open(io.BytesIO(img_data))\n",
    "    # Convert to bytes for API submission\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    img_bytes = buffered.getvalue()\n",
    "    return base64.b64encode(img_bytes).decode('utf-8')\n",
    " \n",
    "def process_page_with_vision_api(page_image_base64, page_num):\n",
    "    \"\"\"Process a page image with OpenAI Vision API\"\"\"\n",
    "    # Robust prompt designed for document OCR with table preservation\n",
    "    prompt = \"\"\"\n",
    "    Perform Optical Character Recognition (OCR) on this document image with these specific requirements:\n",
    "    1. Extract ALL text content exactly as it appears, preserving original structure.\n",
    "    2. For tables:\n",
    "       - Maintain original table layout with proper alignment\n",
    "       - Keep column headers and row data properly aligned\n",
    "       - Use spaces or tabs to preserve column alignment\n",
    "       - Preserve all numbers, decimal points, and special characters in tables\n",
    "    3. Include ALL text elements:\n",
    "       - Headings and subheadings\n",
    "       - Body text paragraphs\n",
    "       - Footnotes, captions, and annotations\n",
    "       - Numbers, dates, and currency values (maintain original format)\n",
    "       - Lists and bullet points\n",
    "    4. Preserve the reading order from top to bottom, left to right\n",
    "    5. Indicate page headers/footers by adding [HEADER] or [FOOTER] tags\n",
    "    6. Mark any disclaimers or notes with [NOTE] tags\n",
    "    7. If text appears in multiple columns, process each column completely before moving to the next\n",
    "    Focus on accurate text extraction with proper layout preservation, especially for tabular data.\n",
    "    DO NOT summarize, interpret, or modify the content.\n",
    "    DO NOT add any commentary or explanations.\n",
    "    \"\"\"\n",
    " \n",
    "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a precise OCR system that perfectly extracts text while maintaining document structure.\"},\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{page_image_base64}\",\n",
    "                                \"detail\": \"high\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=20000\n",
    "        )\n",
    "        # Extract OCR text from response\n",
    "        ocr_text = response.choices[0].message.content\n",
    "        return ocr_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing page {page_num}: {str(e)}\")\n",
    "        return f\"[ERROR PROCESSING PAGE {page_num}]: {str(e)}\"\n",
    " \n",
    "def process_page(pdf_path, page_num, pages_dir):\n",
    "    \"\"\"Process a single page: extract as image and perform OCR\"\"\"\n",
    "    print(f\"Processing page {page_num+1}...\")\n",
    "    try:\n",
    "        # Get page as base64 image\n",
    "        page_image_base64 = extract_page_as_image(pdf_path, page_num)\n",
    "        # Get OCR text from Vision API\n",
    "        ocr_text = process_page_with_vision_api(page_image_base64, page_num+1)\n",
    "        # Save page OCR to file\n",
    "        page_file = os.path.join(pages_dir, f\"page_{page_num+1:03d}.txt\")\n",
    "        with open(page_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(ocr_text)\n",
    "        return page_num+1, ocr_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in page {page_num+1} processing: {str(e)}\")\n",
    "        return page_num+1, f\"[ERROR ON PAGE {page_num+1}]: {str(e)}\"\n",
    " \n",
    "def process_pdf(pdf_path):\n",
    "    \"\"\"Process the entire PDF document in parallel\"\"\"\n",
    "    output_dir, pages_dir = setup_output_directory(pdf_path)\n",
    "    # Get number of pages\n",
    "    doc = fitz.open(pdf_path)\n",
    "    num_pages = len(doc)\n",
    "    print(f\"PDF has {num_pages} pages. Starting OCR extraction...\")\n",
    "    # Process pages in parallel\n",
    "    results = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # Submit jobs\n",
    "        future_to_page = {\n",
    "            executor.submit(process_page, pdf_path, page_num, pages_dir): page_num \n",
    "            for page_num in range(num_pages)\n",
    "        }\n",
    "        # Collect results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_page):\n",
    "            page_num = future_to_page[future]\n",
    "            try:\n",
    "                page_num, ocr_text = future.result()\n",
    "                results[page_num] = ocr_text\n",
    "                print(f\"✅ Completed page {page_num}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Page {page_num+1} generated an exception: {str(e)}\")\n",
    "    # Combine all pages in order\n",
    "    combined_text = \"\"\n",
    "    for page_num in sorted(results.keys()):\n",
    "        combined_text += f\"\\n\\n----- PAGE {page_num} -----\\n\\n\"\n",
    "        combined_text += results[page_num]\n",
    "    # Save combined OCR text\n",
    "    combined_file = os.path.join(output_dir, \"combined_ocr.txt\")\n",
    "    with open(combined_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(combined_text)\n",
    "    print(f\"\\nOCR extraction complete!\")\n",
    "    print(f\"- Individual page files saved in: {pages_dir}\")\n",
    "    print(f\"- Combined OCR text saved to: {combined_file}\")\n",
    "    return combined_file\n",
    " \n",
    "def main():\n",
    "    \"\"\"Main function to run the OCR extraction\"\"\"\n",
    "    # Get PDF file path from user\n",
    "    pdf_path = input(\"Enter the path to your PDF file: \")\n",
    "    # Validate path\n",
    "    if not os.path.exists(pdf_path) or not pdf_path.lower().endswith('.pdf'):\n",
    "        print(\"Invalid PDF file path. Please provide a valid PDF file.\")\n",
    "        return\n",
    "    # Process the PDF\n",
    "    start_time = time.time()\n",
    "    combined_file = process_pdf(pdf_path)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nProcessing completed in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Combined OCR text saved to: {combined_file}\")\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff60b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import psycopg2\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime, date\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection parameters\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST'),\n",
    "    'user': os.getenv('DB_USER'),\n",
    "    'password': os.getenv('DB_PASSWORD'),\n",
    "    'database': os.getenv('DB_NAME'),\n",
    "    'port': os.getenv('DB_PORT')\n",
    "}\n",
    "\n",
    "# GPT-4.1 API configuration\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n",
    "\n",
    "API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "MODEL = \"gpt-4.1\"\n",
    "\n",
    "def date_serializer(obj):\n",
    "    \"\"\"Custom JSON serializer for handling date objects.\"\"\"\n",
    "    if isinstance(obj, date):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {obj.__class__.__name__} not serializable\")\n",
    "\n",
    "def read_ocr_file(filename: str) -> str:\n",
    "    \"\"\"Read OCR output file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(filename, \"r\", encoding=\"latin-1\") as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file with latin-1 encoding: {e}\")\n",
    "            exit(1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"OCR file {filename} not found.\")\n",
    "        exit(1)\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Get a connection to the PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Database connection error: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "def generate_json_from_data(data_content: str, schema_definition: str) -> Dict[str, Any]:\n",
    "    \"\"\"Generate JSON from data content using GPT-4.1.\"\"\"\n",
    "    \n",
    "    # Prepare the prompt for GPT-4.1\n",
    "    prompt = f\"\"\"\n",
    "    I need to convert structured data into a JSON format that follows a PostgreSQL database schema.\n",
    "    \n",
    "    Here's the database schema:\n",
    "    {schema_definition}\n",
    "    \n",
    "    Here's the data that needs to be converted to JSON:\n",
    "    {data_content}\n",
    "    \n",
    "    Generate a JSON structure that follows the database schema tables. Extract all relevant information from the data.\n",
    "    \n",
    "    IMPORTANT GUIDELINES:\n",
    "    1. Follow the schema structure precisely (batches, steps, facts, materials, signoffs).\n",
    "    2. Use the exact field names as specified in the schema.\n",
    "    3. Ensure all foreign key relationships are maintained correctly.\n",
    "    4. Format dates as YYYY-MM-DD strings.\n",
    "    5. For jsonb fields (like 'kv'), use nested JSON objects.\n",
    "    6. For fact categories, use appropriate values like 'measurement', 'answer', 'calculation', etc.\n",
    "    7. Format the output as a valid JSON object.\n",
    "    \n",
    "    The JSON structure should have these top-level keys:\n",
    "    - batches: Array of batch objects\n",
    "    - steps: Array of step objects \n",
    "    - facts: Array of fact objects\n",
    "    - materials: Array of material objects\n",
    "    - signoffs: Array of signoff objects\n",
    "    \n",
    "    Return ONLY the JSON structure without any explanation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make API call to GPT-4.1\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.2  # Lower temperature for more consistent output\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        # Extract the JSON from the response\n",
    "        json_text = result['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        # Clean up any markdown formatting if present\n",
    "        if json_text.startswith(\"```json\"):\n",
    "            json_text = json_text[7:]\n",
    "        if json_text.endswith(\"```\"):\n",
    "            json_text = json_text[:-3]\n",
    "        \n",
    "        json_text = json_text.strip()\n",
    "        \n",
    "        # Parse the JSON\n",
    "        return json.loads(json_text)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request error: {e}\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error: {e}\")\n",
    "        print(f\"Response content: {json_text}\")\n",
    "        return {}\n",
    "\n",
    "def insert_into_database(data: Dict[str, Any]) -> None:\n",
    "    \"\"\"Insert JSON data into PostgreSQL database.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Insert batches\n",
    "        if \"batches\" in data:\n",
    "            for batch in data[\"batches\"]:\n",
    "                batch_id = batch.get(\"batch_id\")\n",
    "                \n",
    "                # Check if batch already exists\n",
    "                cursor.execute(\"SELECT batch_id FROM batches WHERE batch_id = %s\", (batch_id,))\n",
    "                if cursor.fetchone() is None:\n",
    "                    # Insert new batch\n",
    "                    cursor.execute(\n",
    "                        \"INSERT INTO batches (batch_id, doc_no, title, effective_dt) VALUES (%s, %s, %s, %s)\",\n",
    "                        (batch_id, batch.get(\"doc_no\"), batch.get(\"title\"), batch.get(\"effective_dt\"))\n",
    "                    )\n",
    "                    print(f\"Inserted batch: {batch_id}\")\n",
    "                else:\n",
    "                    print(f\"Batch {batch_id} already exists, skipping.\")\n",
    "        \n",
    "        # Insert steps\n",
    "        step_id_map = {}  # Map to store original step_id to new step_id\n",
    "        if \"steps\" in data:\n",
    "            for step in data[\"steps\"]:\n",
    "                original_step_id = step.get(\"step_id\")\n",
    "                batch_id = step.get(\"batch_id\")\n",
    "                step_no = step.get(\"step_no\")\n",
    "                \n",
    "                # Check if step already exists\n",
    "                cursor.execute(\"SELECT step_id FROM steps WHERE batch_id = %s AND step_no = %s\", (batch_id, step_no))\n",
    "                result = cursor.fetchone()\n",
    "                \n",
    "                if result is None:\n",
    "                    # Insert new step\n",
    "                    cursor.execute(\n",
    "                        \"INSERT INTO steps (batch_id, step_no, step_title) VALUES (%s, %s, %s) RETURNING step_id\",\n",
    "                        (batch_id, step_no, step.get(\"step_title\"))\n",
    "                    )\n",
    "                    new_step_id = cursor.fetchone()[0]\n",
    "                    step_id_map[original_step_id] = new_step_id\n",
    "                    print(f\"Inserted step: {step_no} (ID: {new_step_id})\")\n",
    "                else:\n",
    "                    step_id_map[original_step_id] = result[0]\n",
    "                    print(f\"Step {step_no} already exists (ID: {result[0]}), skipping.\")\n",
    "        \n",
    "        # Insert materials\n",
    "        if \"materials\" in data:\n",
    "            for material in data[\"materials\"]:\n",
    "                original_step_id = material.get(\"step_id\")\n",
    "                step_id = step_id_map.get(original_step_id)\n",
    "                \n",
    "                if step_id:\n",
    "                    cursor.execute(\n",
    "                        \"INSERT INTO materials (step_id, kv) VALUES (%s, %s)\",\n",
    "                        (step_id, json.dumps(material.get(\"kv\")))\n",
    "                    )\n",
    "                    print(f\"Inserted material for step_id: {step_id}\")\n",
    "        \n",
    "        # Insert facts\n",
    "        if \"facts\" in data:\n",
    "            for fact in data[\"facts\"]:\n",
    "                original_step_id = fact.get(\"step_id\")\n",
    "                step_id = step_id_map.get(original_step_id)\n",
    "                \n",
    "                if step_id:\n",
    "                    cursor.execute(\n",
    "                        \"INSERT INTO facts (step_id, category, kv) VALUES (%s, %s, %s)\",\n",
    "                        (step_id, fact.get(\"category\"), json.dumps(fact.get(\"kv\")))\n",
    "                    )\n",
    "                    print(f\"Inserted fact for step_id: {step_id}\")\n",
    "        \n",
    "        # Insert signoffs\n",
    "        if \"signoffs\" in data:\n",
    "            for signoff in data[\"signoffs\"]:\n",
    "                original_step_id = signoff.get(\"step_id\")\n",
    "                step_id = step_id_map.get(original_step_id)\n",
    "                signed_ts = signoff.get(\"signed_ts\")\n",
    "                \n",
    "                if step_id:\n",
    "                    cursor.execute(\n",
    "                        \"INSERT INTO signoffs (step_id, role, initials, signed_ts) VALUES (%s, %s, %s, %s)\",\n",
    "                        (step_id, signoff.get(\"role\"), signoff.get(\"initials\"), signed_ts)\n",
    "                    )\n",
    "                    print(f\"Inserted signoff for step_id: {step_id}\")\n",
    "        \n",
    "        # Commit all changes to the database\n",
    "        conn.commit()\n",
    "        print(\"All data successfully inserted into the database!\")\n",
    "    \n",
    "    except psycopg2.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if conn:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "def main():\n",
    "    # Define database schema\n",
    "    schema_definition = \"\"\"\n",
    "    CREATE TABLE batches (\n",
    "        batch_id      text PRIMARY KEY,\n",
    "        doc_no        text,\n",
    "        title         text,\n",
    "        effective_dt  date\n",
    "    );\n",
    "    CREATE TABLE steps (\n",
    "        step_id       bigserial PRIMARY KEY,\n",
    "        batch_id      text REFERENCES batches,\n",
    "        step_no       text,           -- '5.2', '3.4', '6.1', …\n",
    "        step_title    text\n",
    "    );\n",
    "    CREATE TABLE facts (           -- \"wide-fact\" table\n",
    "        fact_id       bigserial PRIMARY KEY,\n",
    "        step_id       bigint REFERENCES steps,\n",
    "        category      text,         -- measurement | answer | setting\n",
    "        kv            jsonb         -- free-form key/value payload\n",
    "    );\n",
    "    CREATE TABLE materials (\n",
    "        mat_id        bigserial PRIMARY KEY,\n",
    "        step_id       bigint REFERENCES steps,\n",
    "        kv            jsonb         -- {name, part_no, lot, qty, unit, expiry …}\n",
    "    );\n",
    "    CREATE TABLE signoffs (\n",
    "        sign_id       bigserial PRIMARY KEY,\n",
    "        step_id       bigint REFERENCES steps,\n",
    "        role          text,         -- performer | verifier | supervisor\n",
    "        initials      text,\n",
    "        signed_ts     timestamp\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read data content from file\n",
    "    data_content = read_ocr_file(r\"EBR-06-922947-PUR-HID2007_922945_235_A Prep and  Sartobind Q_ocr_output\\EBR-06-922947-PUR-HID2007_922945_235_A Prep and  Sartobind Q_ocr_output.txt\")\n",
    "    \n",
    "    # Generate JSON from data\n",
    "    print(\"Generating JSON from data using GPT-4.1...\")\n",
    "    json_data = generate_json_from_data(data_content, schema_definition)\n",
    "    \n",
    "    # Save JSON to file for reference\n",
    "    with open(\"generated_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, indent=2, default=date_serializer)\n",
    "    print(\"JSON data saved to 'generated_data.json'\")\n",
    "    \n",
    "    # Insert data into database\n",
    "    print(\"Inserting data into database...\")\n",
    "    insert_into_database(json_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
